{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    \n",
    "    def __init__(self,n_visible,n_hidden, particles = 30, beta=1,precision=64):\n",
    "        if precision == 64:\n",
    "            self.np_type = np.float64\n",
    "        elif precision == 32:\n",
    "            self.np_type = np.float32\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.particles = particles\n",
    "        # initialize weights\n",
    "        self.W = np.random.normal(loc = 0, scale = 0.01, size = (n_visible, n_hidden)).astype(self.np_type)\n",
    "        # initialize bias for visible layer\n",
    "        self.b = np.zeros(shape = (1,n_visible), dtype=self.np_type)\n",
    "        # initialize bias for hidden layer\n",
    "        self.c = np.zeros(shape = (1,n_hidden), dtype=self.np_type)\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.N = np.random.randint(low=0, high=2, size=(particles,self.n_visible)).astype(int)\n",
    "        self.global_step = 0\n",
    "        \n",
    "    def getParameters(self):\n",
    "        params = {}\n",
    "        params['W'] = self.W\n",
    "        params['b'] = self.b\n",
    "        params['c'] = self.c\n",
    "        return params\n",
    "    \n",
    "    def sample(self, iterations = 100, size = 1):\n",
    "        return self.sampleFrom(np.random.randint(low=0, high=2, size=(size,self.n_visible)), iterations = iterations, size = size)\n",
    "\n",
    "    #\n",
    "    # Sample from the learned distribution, starting at some\n",
    "    # initial value\n",
    "    #\n",
    "    def sampleFrom(self, initial, iterations = 100, size = 1):\n",
    "        V = initial.astype(int)\n",
    "        for i in range(iterations):\n",
    "            V, _ = self.sampleGibbs(V, size = size)\n",
    "            if (iterations > 1000):\n",
    "                if 0 == i % 1000:\n",
    "                    print(\"Sampling iteration \", i)\n",
    "        return V\n",
    "        \n",
    "    def sampleGibbs(self, V, size = 1):\n",
    "        \"\"\"\n",
    "        Run one-step Gibbs sampling\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Sample hidden units from visible units\n",
    "        # compute expected value of hidden units given visible units\n",
    "        E_h = expit(self.beta*(V.astype(int)@self.W + self.c), dtype=self.np_type)\n",
    "        # random ramples from uniform distribution U(0,1)\n",
    "        U = np.random.random_sample(size=(size, self.n_hidden)).astype(self.np_type)\n",
    "        # hidden units\n",
    "        H = (U <= E_h).astype(int)\n",
    "        \n",
    "        E_v = expit(self.beta*(H@np.transpose(self.W) + self.b), dtype=self.np_type)\n",
    "        U = np.random.random_sample(size=(size, self.n_visible)).astype(self.np_type)\n",
    "        Vb = (U <= E_v).astype(int)\n",
    "        \n",
    "        return Vb, E_h\n",
    "        \n",
    "        \n",
    "    def train_CD(self, V, iterations = 100, epochs = 1, step = 0.01, weigth_decay=0):\n",
    "        \n",
    "        batch_size = V.shape[0]\n",
    "        if V.shape[1] != self.n_visible:\n",
    "            print(\"Shape of training data\", V.shape)\n",
    "            raise ValueError(\"Data does not match number of visible units.\")\n",
    "            \n",
    "        # logs\n",
    "        dw = []\n",
    "        errors = []\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            # run one Gibss sampling and obtain new values\n",
    "            # for visible units and previous expectation values\n",
    "            Vb, E = self.sampleGibbs(V, batch_size)\n",
    "            \n",
    "            # Calculate new expectation values\n",
    "            Eb = expit(self.beta*(Vb@self.W + self.c), dtype=self.np_type)\n",
    "            \n",
    "            # Calculate contributions of positive and negative phase\n",
    "            # and update weights and bias\n",
    "            \n",
    "            pos = np.tensordot(V,E,axes=((0),(0))).astype(self.np_type) # data\n",
    "            neg = np.tensordot(Vb, Eb, axes=((0),(0))).astype(self.np_type) # model\n",
    "            \n",
    "            # weight update\n",
    "            dW = step*self.beta*(pos -neg) / float(batch_size)\n",
    "            self.W += dW\n",
    "            # bias update\n",
    "            self.b += step*self.beta*np.sum(V - Vb, 0) / float(batch_size)\n",
    "            self.c += step*self.beta*np.sum(E - Eb, 0) / float(batch_size)\n",
    "            \n",
    "            # update logs\n",
    "            dw.append(np.linalg.norm(dW))\n",
    "            recon_error = np.linalg.norm(V-Vb)\n",
    "            errors.append(recon_error)\n",
    "            if 0 == (self.global_step % 500):\n",
    "                print(\"Iteration \", self.global_step,\" - reconstruction error is now\", recon_error)\n",
    "            self.global_step +=1 \n",
    "        return dw, errors\n",
    "    \n",
    "    def train_PCD(self, V, iterations=10000, epochs=1, step=0.001, weight_decay=0.0001):\n",
    "        #check shape\n",
    "        batch_size = V.shape[0]\n",
    "        if (V.shape[1] != self.n_visible):\n",
    "            print(\"Shape of training data\", V.shape)\n",
    "            raise ValueError(\"Data does not match number of visible units\")\n",
    "        initial_step_size = step\n",
    "        \n",
    "        # logs\n",
    "        dw = []\n",
    "        errors = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # update step size\n",
    "            step = initial_step_size * (1.0-(1.0*self.global_step)/(1.0*iterations*epochs))\n",
    "            # compute negative phase\n",
    "            self.N, _ = self.sampleGibbs(self.N, size=self.particles)\n",
    "            # use this to caculate the negative phase\n",
    "            Eb = expit(self.beta*(self.N@self.W + self.c), dtype=self.np_type)\n",
    "            neg = np.tensordot(self.N,Eb,axes=((0),(0))).astype(self.np_type)\n",
    "            # compute the positive phase\n",
    "            E_h = expit(self.beta*(V@self.W + self.c))\n",
    "            pos = np.tensordot(V, E_h, axes=((0),(0)))\n",
    "            # update weights\n",
    "            dW = step*self.beta*(pos -neg) / float(batch_size) - step*weight_decay*self.W / float(batch_size)\n",
    "            self.W += dW\n",
    "            self.b += step*self.beta*np.sum(V - self.N, 0) / float(batch_size) \n",
    "            self.c += step*self.beta*np.sum(E_h - Eb, 0) / float(batch_size) \n",
    "            \n",
    "            if 0 == (self.global_step % 50):\n",
    "                Vb = self.sampleFrom(initial = V, size=batch_size, iterations = 1)\n",
    "                recon_error = np.linalg.norm(V - Vb) \n",
    "                errors.append(recon_error)\n",
    "                if 0 == (self.global_step % 500):\n",
    "                    print(\"Iteration \",self.global_step,\"recon error is \", recon_error)\n",
    "            self.global_step +=1\n",
    "        return dw, errors\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "mndata = MNIST('./')\n",
    "x_train, y_train = mndata.load_training()\n",
    "x_test, y_test = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.int32)\n",
    "x_test = np.asarray(x_test).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = RBM(784,784)\n",
    "thr = 100\n",
    "rbm.particles = thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.zeros((thr,784))\n",
    "label = 0\n",
    "num = 0\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    if y_train[i] == label:\n",
    "        V[num,:] = x_train[i]/255\n",
    "        num += 1\n",
    "    if num >= thr:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0  - reconstruction error is now 195.7442066573728\n"
     ]
    }
   ],
   "source": [
    "dw, errors = rbm.train_CD(V=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = rbm.sample()\n",
    "fig = plt.figure(frameon=False)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "plt.imshow(sample.reshape([28,28]))\n",
    "fig.savefig(\"out1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = V[1,:].reshape([28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(frameon=False)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "plt.imshow(temp)\n",
    "fig.savefig(\"data0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
