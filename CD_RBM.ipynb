{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    \n",
    "    def __init__(self,n_visible,n_hidden, particles = thr, beta=1,precision=64):\n",
    "        if precision == 64:\n",
    "            self.np_type = np.float64\n",
    "        elif precision == 32:\n",
    "            self.np_type = np.float32\n",
    "        self.n_visible = n_visible # number of visible units\n",
    "        self.n_hidden = n_hidden # number of hidden units\n",
    "        self.particles = particles\n",
    "        # initialize weights\n",
    "        self.W = np.random.normal(loc = 0, scale = 0.01, size = (n_visible, n_hidden)).astype(self.np_type)\n",
    "        # initialize bias for visible layer\n",
    "        self.b = np.zeros(shape = (1,n_visible), dtype=self.np_type)\n",
    "        # initialize bias for hidden layer\n",
    "        self.c = np.zeros(shape = (1,n_hidden), dtype=self.np_type)\n",
    "        \n",
    "        self.beta = beta # temparature\n",
    "        self.N = np.random.randint(low=0, high=2, size=(particles,self.n_visible)).astype(int)\n",
    "        self.global_step = 0\n",
    "        self.W_hist = []\n",
    "        \n",
    "        \n",
    "    def getParameters(self):\n",
    "        params = {}\n",
    "        params['W'] = self.W\n",
    "        params['b'] = self.b\n",
    "        params['c'] = self.c\n",
    "        return params\n",
    "    \n",
    "    def setParameters(self, W, b, c):\n",
    "        # checks shapes of parameters\n",
    "        if W.shape != self.W.shape:\n",
    "            raise ValueError(\"The size of weights is invalid.\")\n",
    "        if b.shape != self.b.shape:\n",
    "            raise ValueError(\"The size of bias for visible units is invalid.\")\n",
    "        if c.shape != self.c.shape:\n",
    "            raise ValueError(\"The size of bias for hidden units is invalid.\")\n",
    "        \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "    \n",
    "    def sample(self, iterations = 100, size = 1):\n",
    "        return self.sampleFrom(np.random.randint(low=0, high=2, size=(size,self.n_visible)), iterations = iterations, size = size)\n",
    "\n",
    "    #\n",
    "    # Sample from the learned distribution, starting at some\n",
    "    # initial value\n",
    "    #\n",
    "    def sampleFrom(self, initial, iterations = 100, size = 1):\n",
    "        V = initial.astype(int)\n",
    "        for i in range(iterations):\n",
    "            V, _ = self.sampleGibbs(V, size = size)\n",
    "            if (iterations > 1000):\n",
    "                if 0 == i % 1000:\n",
    "                    print(\"Sampling iteration \", i)\n",
    "        return V\n",
    "        \n",
    "    def sampleGibbs(self, V, size = 1):\n",
    "        \"\"\"\n",
    "        Run one-step Gibbs sampling\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Sample hidden units from visible units\n",
    "        # compute expected value of hidden units given visible units\n",
    "        E_h = expit(self.beta*(V.astype(int)@self.W + self.c), dtype=self.np_type)\n",
    "        # random samples from uniform distribution U(0,1)\n",
    "        U = np.random.random_sample(size=(size, self.n_hidden)).astype(self.np_type)\n",
    "        # updates hidden units\n",
    "        H = (U <= E_h).astype(int)\n",
    "        \n",
    "        # compute expected value of visible units given the hidden units\n",
    "        E_v = expit(self.beta*(H@np.transpose(self.W) + self.b), dtype=self.np_type)\n",
    "        U = np.random.random_sample(size=(size, self.n_visible)).astype(self.np_type)\n",
    "        # update visible units\n",
    "        Vb = (U <= E_v).astype(int)\n",
    "        \n",
    "        return Vb, E_h\n",
    "        \n",
    "        \n",
    "    def train_CD(self, V, iterations = 100, epochs = 1, step = 0.01, weigth_decay=0.0001):\n",
    "        \"\"\"\n",
    "        Contrastive Divergence \n",
    "        \"\"\"\n",
    "        batch_size = V.shape[0]\n",
    "        if V.shape[1] != self.n_visible:\n",
    "            print(\"Shape of training data\", V.shape)\n",
    "            raise ValueError(\"Data does not match number of visible units.\")\n",
    "            \n",
    "        # logs\n",
    "        dw = []\n",
    "        errors = []\n",
    "        self.W_hist = np.zeros((self.W.shape[0],self.W.shape[1],iterations+1))\n",
    "        self.W_hist[:,:,0] = self.W\n",
    "        for i in range(iterations):\n",
    "            # run one Gibss sampling and obtain new values\n",
    "            # for visible units and previous expectation values\n",
    "            Vb, E = self.sampleGibbs(V, batch_size)\n",
    "            \n",
    "            # Calculate new expectation values\n",
    "            Eb = expit(self.beta*(Vb@self.W + self.c), dtype=self.np_type)\n",
    "            \n",
    "            # Calculate contributions of positive and negative phase\n",
    "            # and update weights and bias\n",
    "            \n",
    "            pos = np.tensordot(V,E,axes=((0),(0))).astype(self.np_type) # data\n",
    "            neg = np.tensordot(Vb, Eb, axes=((0),(0))).astype(self.np_type) # model\n",
    "            \n",
    "            # weight update\n",
    "            dW = step*self.beta*(pos -neg) / float(batch_size)\n",
    "            self.W += dW\n",
    "            # bias update\n",
    "            self.b += step*self.beta*np.sum(V - Vb, 0) / float(batch_size)\n",
    "            self.c += step*self.beta*np.sum(E - Eb, 0) / float(batch_size)\n",
    "            self.W_hist[:,:,i+1] = self.W\n",
    "            # update logs\n",
    "            dw.append(np.linalg.norm(dW))\n",
    "            recon_error = np.linalg.norm(V-Vb)\n",
    "            errors.append(recon_error)\n",
    "            if 0 == (self.global_step % 100):\n",
    "                print(\"Iteration \", self.global_step,\" - reconstruction error is now\", recon_error)\n",
    "            self.global_step +=1 \n",
    "        return dw, errors\n",
    "    \n",
    "    def train_PCD(self, V, iterations=10000, epochs=1,step = 0.01,lr=0.001, weight_decay=0.0001):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - V:\n",
    "            - iterations: \n",
    "            - epochs: \n",
    "            - lr: learning rate (default is 0.001)\n",
    "            - weight_decay : the strength of L2 norm (default is 0.0001)\n",
    "        Output: \n",
    "            - dw : \n",
    "            - errors : \n",
    "        \"\"\"\n",
    "        #check shape\n",
    "        batch_size = V.shape[0]\n",
    "        if (V.shape[1] != self.n_visible):\n",
    "            print(\"Shape of training data\", V.shape)\n",
    "            raise ValueError(\"Data does not match number of visible units\")\n",
    "        initial_step_size = step\n",
    "        \n",
    "        # logs\n",
    "        dw = []\n",
    "        errors = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # update step size\n",
    "            step = initial_step_size * (1.0-(1.0*self.global_step)/(1.0*iterations*epochs))\n",
    "            # compute negative phase\n",
    "            self.N, _ = self.sampleGibbs(self.N, size=self.particles)\n",
    "            # use this to caculate the negative phase\n",
    "            Eb = expit(self.beta*(self.N@self.W + self.c), dtype=self.np_type)\n",
    "            neg = np.tensordot(self.N,Eb,axes=((0),(0))).astype(self.np_type)\n",
    "            # compute the positive phase\n",
    "            E_h = expit(self.beta*(V@self.W + self.c))\n",
    "            pos = np.tensordot(V, E_h, axes=((0),(0)))\n",
    "            # update weights w_{t+1} = w_{t} + \n",
    "            dW = step*self.beta*(pos -neg) / float(batch_size) - step*weight_decay*self.W / float(batch_size)\n",
    "            self.W += dW\n",
    "            self.b += step*self.beta*np.sum(V - self.N, 0) / float(batch_size) \n",
    "            self.c += step*self.beta*np.sum(E_h - Eb, 0) / float(batch_size) \n",
    "            \n",
    "            if 0 == (self.global_step % 50):\n",
    "                Vb = self.sampleFrom(initial = V, size=batch_size, iterations = 1)\n",
    "                recon_error = np.linalg.norm(V - Vb) \n",
    "                errors.append(recon_error)\n",
    "                if 0 == (self.global_step % 500):\n",
    "                    print(\"Iteration \",self.global_step,\"recon error is \", recon_error)\n",
    "            self.global_step +=1\n",
    "        return dw, errors\n",
    "    \n",
    "    \"\"\"\n",
    "    def comput_likelihood(self, V):\n",
    "        This function computes the likelihood given the data\n",
    "        Input: \n",
    "            - V: the set of images\n",
    "        Outpu:\n",
    "            - lik : the likelihood\n",
    "        # L(\\theta) = \\prod_{i=1}^{n}p(v_i|h,\\theta)\n",
    "        lik = \n",
    "        \n",
    "    def plot_error(self):\n",
    "    \"\"\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "mndata = MNIST('./')\n",
    "x_train, y_train = mndata.load_training()\n",
    "x_test, y_test = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.int32)\n",
    "x_test = np.asarray(x_test).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 100\n",
    "rbm = RBM(784,784,thr)\n",
    "rbm.particles = thr\n",
    "\n",
    "V = np.zeros((thr,784))\n",
    "labels = [3]#[0,1,2,3,4,5,6,7,8,9]\n",
    "num = 0\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    if y_train[i] in labels:\n",
    "        V[num,:] = x_train[i]/255\n",
    "        num += 1\n",
    "    if num >= thr:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0  - reconstruction error is now 194.61949153391254\n"
     ]
    }
   ],
   "source": [
    "dw, errors = rbm.train_CD(V=V,iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = rbm.sample()\n",
    "fig = plt.figure(frameon=False)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "plt.imshow(sample.reshape([28,28]))\n",
    "fig.savefig(\"cd3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = V[1,:].reshape([28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x821206f98>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(frameon=False)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "plt.imshow(temp)\n",
    "#fig.savefig(\"data0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 784, 1001)\n"
     ]
    }
   ],
   "source": [
    "W_hist = rbm.W_hist\n",
    "print(W_hist.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
